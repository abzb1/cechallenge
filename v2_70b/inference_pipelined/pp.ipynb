{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c5b954-2998-4c22-b59f-9ed1414562ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from model import get_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f78ab68-967c-41ab-a4f0-5459dbdad30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_model_dict = torch.load(\"../whole_llama2_model.pt\")\n",
    "whole_model_keys_lst = list(whole_model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cc18d1-02a6-437c-b060-ff54bcf41d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight torch.Size([32000, 8192])\n",
      "norm.weight torch.Size([8192])\n",
      "output.weight torch.Size([32000, 8192])\n",
      "layers.0.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.0.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.0.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.0.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.0.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.0.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.0.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.0.attention_norm.weight torch.Size([8192])\n",
      "layers.0.ffn_norm.weight torch.Size([8192])\n",
      "layers.1.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.1.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.1.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.1.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.1.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.1.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.1.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.1.attention_norm.weight torch.Size([8192])\n",
      "layers.1.ffn_norm.weight torch.Size([8192])\n",
      "layers.2.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.2.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.2.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.2.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.2.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.2.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.2.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.2.attention_norm.weight torch.Size([8192])\n",
      "layers.2.ffn_norm.weight torch.Size([8192])\n",
      "layers.3.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.3.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.3.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.3.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.3.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.3.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.3.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.3.attention_norm.weight torch.Size([8192])\n",
      "layers.3.ffn_norm.weight torch.Size([8192])\n",
      "layers.4.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.4.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.4.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.4.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.4.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.4.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.4.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.4.attention_norm.weight torch.Size([8192])\n",
      "layers.4.ffn_norm.weight torch.Size([8192])\n",
      "layers.5.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.5.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.5.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.5.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.5.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.5.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.5.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.5.attention_norm.weight torch.Size([8192])\n",
      "layers.5.ffn_norm.weight torch.Size([8192])\n",
      "layers.6.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.6.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.6.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.6.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.6.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.6.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.6.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.6.attention_norm.weight torch.Size([8192])\n",
      "layers.6.ffn_norm.weight torch.Size([8192])\n",
      "layers.7.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.7.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.7.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.7.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.7.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.7.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.7.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.7.attention_norm.weight torch.Size([8192])\n",
      "layers.7.ffn_norm.weight torch.Size([8192])\n",
      "layers.8.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.8.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.8.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.8.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.8.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.8.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.8.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.8.attention_norm.weight torch.Size([8192])\n",
      "layers.8.ffn_norm.weight torch.Size([8192])\n",
      "layers.9.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.9.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.9.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.9.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.9.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.9.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.9.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.9.attention_norm.weight torch.Size([8192])\n",
      "layers.9.ffn_norm.weight torch.Size([8192])\n",
      "layers.10.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.10.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.10.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.10.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.10.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.10.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.10.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.10.attention_norm.weight torch.Size([8192])\n",
      "layers.10.ffn_norm.weight torch.Size([8192])\n",
      "layers.11.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.11.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.11.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.11.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.11.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.11.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.11.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.11.attention_norm.weight torch.Size([8192])\n",
      "layers.11.ffn_norm.weight torch.Size([8192])\n",
      "layers.12.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.12.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.12.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.12.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.12.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.12.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.12.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.12.attention_norm.weight torch.Size([8192])\n",
      "layers.12.ffn_norm.weight torch.Size([8192])\n",
      "layers.13.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.13.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.13.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.13.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.13.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.13.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.13.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.13.attention_norm.weight torch.Size([8192])\n",
      "layers.13.ffn_norm.weight torch.Size([8192])\n",
      "layers.14.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.14.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.14.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.14.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.14.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.14.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.14.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.14.attention_norm.weight torch.Size([8192])\n",
      "layers.14.ffn_norm.weight torch.Size([8192])\n",
      "layers.15.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.15.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.15.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.15.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.15.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.15.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.15.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.15.attention_norm.weight torch.Size([8192])\n",
      "layers.15.ffn_norm.weight torch.Size([8192])\n",
      "layers.16.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.16.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.16.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.16.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.16.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.16.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.16.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.16.attention_norm.weight torch.Size([8192])\n",
      "layers.16.ffn_norm.weight torch.Size([8192])\n",
      "layers.17.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.17.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.17.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.17.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.17.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.17.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.17.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.17.attention_norm.weight torch.Size([8192])\n",
      "layers.17.ffn_norm.weight torch.Size([8192])\n",
      "layers.18.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.18.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.18.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.18.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.18.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.18.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.18.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.18.attention_norm.weight torch.Size([8192])\n",
      "layers.18.ffn_norm.weight torch.Size([8192])\n",
      "layers.19.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.19.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.19.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.19.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.19.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.19.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.19.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.19.attention_norm.weight torch.Size([8192])\n",
      "layers.19.ffn_norm.weight torch.Size([8192])\n",
      "layers.20.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.20.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.20.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.20.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.20.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.20.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.20.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.20.attention_norm.weight torch.Size([8192])\n",
      "layers.20.ffn_norm.weight torch.Size([8192])\n",
      "layers.21.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.21.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.21.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.21.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.21.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.21.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.21.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.21.attention_norm.weight torch.Size([8192])\n",
      "layers.21.ffn_norm.weight torch.Size([8192])\n",
      "layers.22.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.22.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.22.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.22.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.22.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.22.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.22.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.22.attention_norm.weight torch.Size([8192])\n",
      "layers.22.ffn_norm.weight torch.Size([8192])\n",
      "layers.23.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.23.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.23.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.23.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.23.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.23.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.23.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.23.attention_norm.weight torch.Size([8192])\n",
      "layers.23.ffn_norm.weight torch.Size([8192])\n",
      "layers.24.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.24.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.24.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.24.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.24.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.24.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.24.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.24.attention_norm.weight torch.Size([8192])\n",
      "layers.24.ffn_norm.weight torch.Size([8192])\n",
      "layers.25.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.25.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.25.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.25.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.25.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.25.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.25.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.25.attention_norm.weight torch.Size([8192])\n",
      "layers.25.ffn_norm.weight torch.Size([8192])\n",
      "layers.26.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.26.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.26.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.26.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.26.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.26.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.26.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.26.attention_norm.weight torch.Size([8192])\n",
      "layers.26.ffn_norm.weight torch.Size([8192])\n",
      "layers.27.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.27.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.27.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.27.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.27.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.27.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.27.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.27.attention_norm.weight torch.Size([8192])\n",
      "layers.27.ffn_norm.weight torch.Size([8192])\n",
      "layers.28.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.28.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.28.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.28.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.28.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.28.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.28.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.28.attention_norm.weight torch.Size([8192])\n",
      "layers.28.ffn_norm.weight torch.Size([8192])\n",
      "layers.29.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.29.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.29.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.29.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.29.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.29.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.29.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.29.attention_norm.weight torch.Size([8192])\n",
      "layers.29.ffn_norm.weight torch.Size([8192])\n",
      "layers.30.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.30.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.30.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.30.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.30.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.30.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.30.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.30.attention_norm.weight torch.Size([8192])\n",
      "layers.30.ffn_norm.weight torch.Size([8192])\n",
      "layers.31.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.31.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.31.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.31.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.31.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.31.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.31.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.31.attention_norm.weight torch.Size([8192])\n",
      "layers.31.ffn_norm.weight torch.Size([8192])\n",
      "layers.32.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.32.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.32.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.32.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.32.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.32.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.32.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.32.attention_norm.weight torch.Size([8192])\n",
      "layers.32.ffn_norm.weight torch.Size([8192])\n",
      "layers.33.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.33.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.33.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.33.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.33.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.33.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.33.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.33.attention_norm.weight torch.Size([8192])\n",
      "layers.33.ffn_norm.weight torch.Size([8192])\n",
      "layers.34.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.34.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.34.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.34.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.34.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.34.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.34.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.34.attention_norm.weight torch.Size([8192])\n",
      "layers.34.ffn_norm.weight torch.Size([8192])\n",
      "layers.35.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.35.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.35.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.35.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.35.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.35.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.35.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.35.attention_norm.weight torch.Size([8192])\n",
      "layers.35.ffn_norm.weight torch.Size([8192])\n",
      "layers.36.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.36.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.36.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.36.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.36.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.36.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.36.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.36.attention_norm.weight torch.Size([8192])\n",
      "layers.36.ffn_norm.weight torch.Size([8192])\n",
      "layers.37.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.37.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.37.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.37.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.37.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.37.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.37.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.37.attention_norm.weight torch.Size([8192])\n",
      "layers.37.ffn_norm.weight torch.Size([8192])\n",
      "layers.38.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.38.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.38.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.38.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.38.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.38.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.38.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.38.attention_norm.weight torch.Size([8192])\n",
      "layers.38.ffn_norm.weight torch.Size([8192])\n",
      "layers.39.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.39.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.39.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.39.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.39.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.39.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.39.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.39.attention_norm.weight torch.Size([8192])\n",
      "layers.39.ffn_norm.weight torch.Size([8192])\n",
      "layers.40.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.40.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.40.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.40.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.40.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.40.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.40.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.40.attention_norm.weight torch.Size([8192])\n",
      "layers.40.ffn_norm.weight torch.Size([8192])\n",
      "layers.41.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.41.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.41.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.41.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.41.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.41.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.41.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.41.attention_norm.weight torch.Size([8192])\n",
      "layers.41.ffn_norm.weight torch.Size([8192])\n",
      "layers.42.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.42.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.42.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.42.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.42.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.42.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.42.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.42.attention_norm.weight torch.Size([8192])\n",
      "layers.42.ffn_norm.weight torch.Size([8192])\n",
      "layers.43.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.43.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.43.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.43.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.43.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.43.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.43.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.43.attention_norm.weight torch.Size([8192])\n",
      "layers.43.ffn_norm.weight torch.Size([8192])\n",
      "layers.44.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.44.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.44.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.44.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.44.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.44.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.44.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.44.attention_norm.weight torch.Size([8192])\n",
      "layers.44.ffn_norm.weight torch.Size([8192])\n",
      "layers.45.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.45.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.45.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.45.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.45.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.45.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.45.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.45.attention_norm.weight torch.Size([8192])\n",
      "layers.45.ffn_norm.weight torch.Size([8192])\n",
      "layers.46.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.46.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.46.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.46.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.46.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.46.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.46.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.46.attention_norm.weight torch.Size([8192])\n",
      "layers.46.ffn_norm.weight torch.Size([8192])\n",
      "layers.47.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.47.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.47.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.47.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.47.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.47.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.47.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.47.attention_norm.weight torch.Size([8192])\n",
      "layers.47.ffn_norm.weight torch.Size([8192])\n",
      "layers.48.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.48.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.48.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.48.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.48.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.48.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.48.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.48.attention_norm.weight torch.Size([8192])\n",
      "layers.48.ffn_norm.weight torch.Size([8192])\n",
      "layers.49.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.49.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.49.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.49.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.49.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.49.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.49.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.49.attention_norm.weight torch.Size([8192])\n",
      "layers.49.ffn_norm.weight torch.Size([8192])\n",
      "layers.50.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.50.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.50.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.50.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.50.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.50.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.50.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.50.attention_norm.weight torch.Size([8192])\n",
      "layers.50.ffn_norm.weight torch.Size([8192])\n",
      "layers.51.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.51.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.51.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.51.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.51.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.51.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.51.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.51.attention_norm.weight torch.Size([8192])\n",
      "layers.51.ffn_norm.weight torch.Size([8192])\n",
      "layers.52.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.52.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.52.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.52.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.52.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.52.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.52.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.52.attention_norm.weight torch.Size([8192])\n",
      "layers.52.ffn_norm.weight torch.Size([8192])\n",
      "layers.53.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.53.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.53.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.53.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.53.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.53.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.53.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.53.attention_norm.weight torch.Size([8192])\n",
      "layers.53.ffn_norm.weight torch.Size([8192])\n",
      "layers.54.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.54.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.54.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.54.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.54.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.54.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.54.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.54.attention_norm.weight torch.Size([8192])\n",
      "layers.54.ffn_norm.weight torch.Size([8192])\n",
      "layers.55.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.55.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.55.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.55.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.55.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.55.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.55.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.55.attention_norm.weight torch.Size([8192])\n",
      "layers.55.ffn_norm.weight torch.Size([8192])\n",
      "layers.56.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.56.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.56.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.56.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.56.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.56.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.56.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.56.attention_norm.weight torch.Size([8192])\n",
      "layers.56.ffn_norm.weight torch.Size([8192])\n",
      "layers.57.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.57.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.57.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.57.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.57.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.57.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.57.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.57.attention_norm.weight torch.Size([8192])\n",
      "layers.57.ffn_norm.weight torch.Size([8192])\n",
      "layers.58.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.58.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.58.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.58.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.58.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.58.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.58.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.58.attention_norm.weight torch.Size([8192])\n",
      "layers.58.ffn_norm.weight torch.Size([8192])\n",
      "layers.59.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.59.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.59.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.59.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.59.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.59.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.59.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.59.attention_norm.weight torch.Size([8192])\n",
      "layers.59.ffn_norm.weight torch.Size([8192])\n",
      "layers.60.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.60.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.60.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.60.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.60.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.60.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.60.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.60.attention_norm.weight torch.Size([8192])\n",
      "layers.60.ffn_norm.weight torch.Size([8192])\n",
      "layers.61.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.61.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.61.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.61.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.61.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.61.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.61.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.61.attention_norm.weight torch.Size([8192])\n",
      "layers.61.ffn_norm.weight torch.Size([8192])\n",
      "layers.62.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.62.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.62.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.62.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.62.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.62.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.62.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.62.attention_norm.weight torch.Size([8192])\n",
      "layers.62.ffn_norm.weight torch.Size([8192])\n",
      "layers.63.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.63.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.63.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.63.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.63.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.63.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.63.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.63.attention_norm.weight torch.Size([8192])\n",
      "layers.63.ffn_norm.weight torch.Size([8192])\n",
      "layers.64.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.64.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.64.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.64.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.64.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.64.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.64.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.64.attention_norm.weight torch.Size([8192])\n",
      "layers.64.ffn_norm.weight torch.Size([8192])\n",
      "layers.65.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.65.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.65.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.65.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.65.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.65.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.65.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.65.attention_norm.weight torch.Size([8192])\n",
      "layers.65.ffn_norm.weight torch.Size([8192])\n",
      "layers.66.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.66.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.66.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.66.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.66.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.66.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.66.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.66.attention_norm.weight torch.Size([8192])\n",
      "layers.66.ffn_norm.weight torch.Size([8192])\n",
      "layers.67.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.67.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.67.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.67.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.67.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.67.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.67.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.67.attention_norm.weight torch.Size([8192])\n",
      "layers.67.ffn_norm.weight torch.Size([8192])\n",
      "layers.68.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.68.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.68.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.68.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.68.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.68.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.68.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.68.attention_norm.weight torch.Size([8192])\n",
      "layers.68.ffn_norm.weight torch.Size([8192])\n",
      "layers.69.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.69.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.69.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.69.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.69.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.69.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.69.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.69.attention_norm.weight torch.Size([8192])\n",
      "layers.69.ffn_norm.weight torch.Size([8192])\n",
      "layers.70.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.70.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.70.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.70.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.70.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.70.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.70.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.70.attention_norm.weight torch.Size([8192])\n",
      "layers.70.ffn_norm.weight torch.Size([8192])\n",
      "layers.71.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.71.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.71.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.71.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.71.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.71.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.71.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.71.attention_norm.weight torch.Size([8192])\n",
      "layers.71.ffn_norm.weight torch.Size([8192])\n",
      "layers.72.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.72.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.72.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.72.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.72.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.72.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.72.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.72.attention_norm.weight torch.Size([8192])\n",
      "layers.72.ffn_norm.weight torch.Size([8192])\n",
      "layers.73.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.73.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.73.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.73.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.73.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.73.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.73.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.73.attention_norm.weight torch.Size([8192])\n",
      "layers.73.ffn_norm.weight torch.Size([8192])\n",
      "layers.74.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.74.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.74.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.74.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.74.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.74.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.74.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.74.attention_norm.weight torch.Size([8192])\n",
      "layers.74.ffn_norm.weight torch.Size([8192])\n",
      "layers.75.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.75.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.75.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.75.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.75.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.75.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.75.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.75.attention_norm.weight torch.Size([8192])\n",
      "layers.75.ffn_norm.weight torch.Size([8192])\n",
      "layers.76.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.76.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.76.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.76.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.76.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.76.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.76.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.76.attention_norm.weight torch.Size([8192])\n",
      "layers.76.ffn_norm.weight torch.Size([8192])\n",
      "layers.77.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.77.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.77.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.77.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.77.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.77.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.77.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.77.attention_norm.weight torch.Size([8192])\n",
      "layers.77.ffn_norm.weight torch.Size([8192])\n",
      "layers.78.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.78.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.78.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.78.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.78.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.78.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.78.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.78.attention_norm.weight torch.Size([8192])\n",
      "layers.78.ffn_norm.weight torch.Size([8192])\n",
      "layers.79.attention.wq.weight torch.Size([8192, 8192])\n",
      "layers.79.attention.wk.weight torch.Size([1024, 8192])\n",
      "layers.79.attention.wv.weight torch.Size([1024, 8192])\n",
      "layers.79.attention.wo.weight torch.Size([8192, 8192])\n",
      "layers.79.feed_forward.w1.weight torch.Size([28672, 8192])\n",
      "layers.79.feed_forward.w2.weight torch.Size([8192, 28672])\n",
      "layers.79.feed_forward.w3.weight torch.Size([28672, 8192])\n",
      "layers.79.attention_norm.weight torch.Size([8192])\n",
      "layers.79.ffn_norm.weight torch.Size([8192])\n",
      "rope.freqs torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for k,v in whole_model_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7108fa44-e7cc-410c-b554-74b739b1f604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 8.6719e-01, 7.5000e-01, 6.4844e-01, 5.6250e-01, 4.8633e-01,\n",
       "        4.2188e-01, 3.6523e-01, 3.1641e-01, 2.7344e-01, 2.3730e-01, 2.0508e-01,\n",
       "        1.7773e-01, 1.5430e-01, 1.3379e-01, 1.1523e-01, 1.0010e-01, 8.6426e-02,\n",
       "        7.5195e-02, 6.4941e-02, 5.6152e-02, 4.8584e-02, 4.2236e-02, 3.6621e-02,\n",
       "        3.1738e-02, 2.7344e-02, 2.3682e-02, 2.0508e-02, 1.7822e-02, 1.5381e-02,\n",
       "        1.3306e-02, 1.1536e-02, 1.0010e-02, 8.6670e-03, 7.5073e-03, 6.5002e-03,\n",
       "        5.6152e-03, 4.8828e-03, 4.2114e-03, 3.6469e-03, 3.1586e-03, 2.7313e-03,\n",
       "        2.3651e-03, 2.0599e-03, 1.7776e-03, 1.5411e-03, 1.3351e-03, 1.1520e-03,\n",
       "        9.9945e-04, 8.6594e-04, 7.5150e-04, 6.4850e-04, 5.6076e-04, 4.8637e-04,\n",
       "        4.2152e-04, 3.6430e-04, 3.1662e-04, 2.7466e-04, 2.3746e-04, 2.0504e-04,\n",
       "        1.7738e-04, 1.5354e-04, 1.3351e-04, 1.1539e-04], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_model_dict[\"rope.freqs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b987a88d-c904-4c45-a578-5ed468f8e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:13<00:00,  1.68s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[model_0(\n",
       "   (tok_embeddings): VocabEmbedding()\n",
       "   (layers): ModuleList(\n",
       "     (0-9): 10 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_1(\n",
       "   (layers): ModuleList(\n",
       "     (0-9): 10 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_2(\n",
       "   (layers): ModuleList(\n",
       "     (0-9): 10 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_3(\n",
       "   (layers): ModuleList(\n",
       "     (0-9): 10 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_4(\n",
       "   (layers): ModuleList(\n",
       "     (0-9): 10 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_5(\n",
       "   (layers): ModuleList(\n",
       "     (0-9): 10 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_6(\n",
       "   (layers): ModuleList(\n",
       "     (0-9): 10 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_7(\n",
       "   (layers): ModuleList(\n",
       "     (0-9): 10 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       "   (norm): RMSNorm()\n",
       "   (output): noParallelLinear()\n",
       " )]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lst = []\n",
    "for i in tqdm(range(8)):\n",
    "    model_lst.append(getattr(get_stage, \"model_\"+str(i))().bfloat16())\n",
    "model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bd58857-577d-4521-8d5d-0d4f87c184ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(i)\n",
    "    state_dict = model_lst[i].state_dict()\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if \"layers\" in k:\n",
    "            new_k = k.lstrip(\"layers.\")\n",
    "            layer_id = int(new_k.split(\".\")[0])\n",
    "            tmp_k = new_k.lstrip(str(layer_id))\n",
    "            layer_id += 10*(i)\n",
    "            new_k = \"layers.\" + str(layer_id) + tmp_k\n",
    "            new_state_dict[k] = whole_model_dict[new_k]\n",
    "        else:\n",
    "            new_state_dict[k] = whole_model_dict[k]\n",
    "    model_lst[i].load_state_dict(new_state_dict)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a403aaf-70ed-4dbe-aab0-5213dfc9f35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.9564e-04,  2.8491e-05,  7.5340e-05,  ..., -2.8729e-05,\n",
       "         -2.9755e-04,  4.8161e-05],\n",
       "        [-1.2207e-04, -7.2861e-04,  1.0834e-03,  ..., -3.7956e-04,\n",
       "          3.8452e-03,  1.9989e-03],\n",
       "        [ 3.3379e-04,  2.6855e-03, -1.8463e-03,  ..., -1.4496e-03,\n",
       "         -6.1646e-03,  1.1169e-02],\n",
       "        ...,\n",
       "        [ 5.4321e-03,  1.8555e-02,  9.0332e-03,  ..., -1.4343e-02,\n",
       "          3.4180e-03, -1.3550e-02],\n",
       "        [-2.7924e-03, -7.9346e-03,  1.4801e-03,  ...,  1.7456e-02,\n",
       "         -1.6235e-02,  1.6724e-02],\n",
       "        [ 1.0498e-02, -6.8283e-04,  2.7771e-03,  ...,  9.9487e-03,\n",
       "         -4.7302e-03,  4.1504e-03]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lst[0].state_dict()['tok_embeddings.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cb9247-9b19-46c8-9314-4b906d425b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [05:26<00:00, 40.84s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, model_ in enumerate(tqdm(model_lst)):\n",
    "    torch.save(model_.state_dict(), \"pp_model/stage_\"+str(i)+\".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cechallenge",
   "language": "python",
   "name": "cechallenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
