{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c5b954-2998-4c22-b59f-9ed1414562ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f78ab68-967c-41ab-a4f0-5459dbdad30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import get_stage\n",
    "\n",
    "whole_model_dict = torch.load(\"../pyllama/whole_llama1_model.pt\")\n",
    "whole_model_keys_lst = list(whole_model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cc18d1-02a6-437c-b060-ff54bcf41d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight torch.Size([32000, 6656])\n",
      "norm.weight torch.Size([6656])\n",
      "output.weight torch.Size([32000, 6656])\n",
      "layers.0.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.0.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.0.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.0.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.0.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.0.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.0.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.0.attention_norm.weight torch.Size([6656])\n",
      "layers.0.ffn_norm.weight torch.Size([6656])\n",
      "layers.1.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.1.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.1.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.1.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.1.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.1.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.1.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.1.attention_norm.weight torch.Size([6656])\n",
      "layers.1.ffn_norm.weight torch.Size([6656])\n",
      "layers.2.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.2.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.2.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.2.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.2.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.2.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.2.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.2.attention_norm.weight torch.Size([6656])\n",
      "layers.2.ffn_norm.weight torch.Size([6656])\n",
      "layers.3.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.3.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.3.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.3.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.3.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.3.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.3.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.3.attention_norm.weight torch.Size([6656])\n",
      "layers.3.ffn_norm.weight torch.Size([6656])\n",
      "layers.4.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.4.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.4.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.4.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.4.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.4.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.4.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.4.attention_norm.weight torch.Size([6656])\n",
      "layers.4.ffn_norm.weight torch.Size([6656])\n",
      "layers.5.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.5.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.5.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.5.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.5.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.5.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.5.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.5.attention_norm.weight torch.Size([6656])\n",
      "layers.5.ffn_norm.weight torch.Size([6656])\n",
      "layers.6.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.6.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.6.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.6.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.6.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.6.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.6.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.6.attention_norm.weight torch.Size([6656])\n",
      "layers.6.ffn_norm.weight torch.Size([6656])\n",
      "layers.7.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.7.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.7.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.7.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.7.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.7.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.7.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.7.attention_norm.weight torch.Size([6656])\n",
      "layers.7.ffn_norm.weight torch.Size([6656])\n",
      "layers.8.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.8.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.8.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.8.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.8.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.8.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.8.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.8.attention_norm.weight torch.Size([6656])\n",
      "layers.8.ffn_norm.weight torch.Size([6656])\n",
      "layers.9.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.9.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.9.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.9.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.9.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.9.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.9.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.9.attention_norm.weight torch.Size([6656])\n",
      "layers.9.ffn_norm.weight torch.Size([6656])\n",
      "layers.10.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.10.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.10.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.10.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.10.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.10.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.10.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.10.attention_norm.weight torch.Size([6656])\n",
      "layers.10.ffn_norm.weight torch.Size([6656])\n",
      "layers.11.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.11.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.11.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.11.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.11.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.11.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.11.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.11.attention_norm.weight torch.Size([6656])\n",
      "layers.11.ffn_norm.weight torch.Size([6656])\n",
      "layers.12.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.12.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.12.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.12.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.12.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.12.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.12.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.12.attention_norm.weight torch.Size([6656])\n",
      "layers.12.ffn_norm.weight torch.Size([6656])\n",
      "layers.13.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.13.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.13.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.13.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.13.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.13.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.13.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.13.attention_norm.weight torch.Size([6656])\n",
      "layers.13.ffn_norm.weight torch.Size([6656])\n",
      "layers.14.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.14.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.14.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.14.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.14.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.14.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.14.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.14.attention_norm.weight torch.Size([6656])\n",
      "layers.14.ffn_norm.weight torch.Size([6656])\n",
      "layers.15.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.15.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.15.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.15.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.15.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.15.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.15.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.15.attention_norm.weight torch.Size([6656])\n",
      "layers.15.ffn_norm.weight torch.Size([6656])\n",
      "layers.16.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.16.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.16.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.16.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.16.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.16.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.16.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.16.attention_norm.weight torch.Size([6656])\n",
      "layers.16.ffn_norm.weight torch.Size([6656])\n",
      "layers.17.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.17.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.17.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.17.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.17.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.17.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.17.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.17.attention_norm.weight torch.Size([6656])\n",
      "layers.17.ffn_norm.weight torch.Size([6656])\n",
      "layers.18.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.18.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.18.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.18.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.18.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.18.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.18.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.18.attention_norm.weight torch.Size([6656])\n",
      "layers.18.ffn_norm.weight torch.Size([6656])\n",
      "layers.19.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.19.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.19.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.19.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.19.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.19.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.19.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.19.attention_norm.weight torch.Size([6656])\n",
      "layers.19.ffn_norm.weight torch.Size([6656])\n",
      "layers.20.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.20.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.20.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.20.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.20.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.20.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.20.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.20.attention_norm.weight torch.Size([6656])\n",
      "layers.20.ffn_norm.weight torch.Size([6656])\n",
      "layers.21.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.21.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.21.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.21.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.21.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.21.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.21.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.21.attention_norm.weight torch.Size([6656])\n",
      "layers.21.ffn_norm.weight torch.Size([6656])\n",
      "layers.22.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.22.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.22.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.22.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.22.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.22.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.22.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.22.attention_norm.weight torch.Size([6656])\n",
      "layers.22.ffn_norm.weight torch.Size([6656])\n",
      "layers.23.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.23.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.23.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.23.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.23.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.23.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.23.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.23.attention_norm.weight torch.Size([6656])\n",
      "layers.23.ffn_norm.weight torch.Size([6656])\n",
      "layers.24.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.24.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.24.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.24.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.24.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.24.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.24.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.24.attention_norm.weight torch.Size([6656])\n",
      "layers.24.ffn_norm.weight torch.Size([6656])\n",
      "layers.25.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.25.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.25.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.25.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.25.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.25.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.25.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.25.attention_norm.weight torch.Size([6656])\n",
      "layers.25.ffn_norm.weight torch.Size([6656])\n",
      "layers.26.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.26.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.26.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.26.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.26.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.26.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.26.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.26.attention_norm.weight torch.Size([6656])\n",
      "layers.26.ffn_norm.weight torch.Size([6656])\n",
      "layers.27.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.27.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.27.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.27.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.27.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.27.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.27.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.27.attention_norm.weight torch.Size([6656])\n",
      "layers.27.ffn_norm.weight torch.Size([6656])\n",
      "layers.28.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.28.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.28.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.28.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.28.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.28.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.28.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.28.attention_norm.weight torch.Size([6656])\n",
      "layers.28.ffn_norm.weight torch.Size([6656])\n",
      "layers.29.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.29.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.29.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.29.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.29.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.29.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.29.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.29.attention_norm.weight torch.Size([6656])\n",
      "layers.29.ffn_norm.weight torch.Size([6656])\n",
      "layers.30.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.30.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.30.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.30.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.30.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.30.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.30.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.30.attention_norm.weight torch.Size([6656])\n",
      "layers.30.ffn_norm.weight torch.Size([6656])\n",
      "layers.31.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.31.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.31.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.31.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.31.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.31.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.31.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.31.attention_norm.weight torch.Size([6656])\n",
      "layers.31.ffn_norm.weight torch.Size([6656])\n",
      "layers.32.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.32.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.32.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.32.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.32.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.32.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.32.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.32.attention_norm.weight torch.Size([6656])\n",
      "layers.32.ffn_norm.weight torch.Size([6656])\n",
      "layers.33.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.33.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.33.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.33.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.33.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.33.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.33.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.33.attention_norm.weight torch.Size([6656])\n",
      "layers.33.ffn_norm.weight torch.Size([6656])\n",
      "layers.34.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.34.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.34.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.34.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.34.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.34.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.34.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.34.attention_norm.weight torch.Size([6656])\n",
      "layers.34.ffn_norm.weight torch.Size([6656])\n",
      "layers.35.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.35.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.35.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.35.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.35.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.35.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.35.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.35.attention_norm.weight torch.Size([6656])\n",
      "layers.35.ffn_norm.weight torch.Size([6656])\n",
      "layers.36.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.36.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.36.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.36.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.36.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.36.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.36.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.36.attention_norm.weight torch.Size([6656])\n",
      "layers.36.ffn_norm.weight torch.Size([6656])\n",
      "layers.37.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.37.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.37.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.37.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.37.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.37.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.37.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.37.attention_norm.weight torch.Size([6656])\n",
      "layers.37.ffn_norm.weight torch.Size([6656])\n",
      "layers.38.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.38.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.38.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.38.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.38.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.38.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.38.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.38.attention_norm.weight torch.Size([6656])\n",
      "layers.38.ffn_norm.weight torch.Size([6656])\n",
      "layers.39.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.39.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.39.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.39.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.39.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.39.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.39.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.39.attention_norm.weight torch.Size([6656])\n",
      "layers.39.ffn_norm.weight torch.Size([6656])\n",
      "layers.40.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.40.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.40.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.40.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.40.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.40.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.40.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.40.attention_norm.weight torch.Size([6656])\n",
      "layers.40.ffn_norm.weight torch.Size([6656])\n",
      "layers.41.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.41.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.41.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.41.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.41.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.41.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.41.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.41.attention_norm.weight torch.Size([6656])\n",
      "layers.41.ffn_norm.weight torch.Size([6656])\n",
      "layers.42.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.42.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.42.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.42.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.42.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.42.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.42.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.42.attention_norm.weight torch.Size([6656])\n",
      "layers.42.ffn_norm.weight torch.Size([6656])\n",
      "layers.43.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.43.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.43.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.43.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.43.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.43.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.43.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.43.attention_norm.weight torch.Size([6656])\n",
      "layers.43.ffn_norm.weight torch.Size([6656])\n",
      "layers.44.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.44.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.44.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.44.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.44.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.44.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.44.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.44.attention_norm.weight torch.Size([6656])\n",
      "layers.44.ffn_norm.weight torch.Size([6656])\n",
      "layers.45.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.45.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.45.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.45.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.45.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.45.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.45.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.45.attention_norm.weight torch.Size([6656])\n",
      "layers.45.ffn_norm.weight torch.Size([6656])\n",
      "layers.46.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.46.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.46.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.46.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.46.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.46.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.46.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.46.attention_norm.weight torch.Size([6656])\n",
      "layers.46.ffn_norm.weight torch.Size([6656])\n",
      "layers.47.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.47.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.47.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.47.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.47.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.47.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.47.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.47.attention_norm.weight torch.Size([6656])\n",
      "layers.47.ffn_norm.weight torch.Size([6656])\n",
      "layers.48.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.48.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.48.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.48.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.48.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.48.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.48.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.48.attention_norm.weight torch.Size([6656])\n",
      "layers.48.ffn_norm.weight torch.Size([6656])\n",
      "layers.49.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.49.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.49.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.49.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.49.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.49.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.49.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.49.attention_norm.weight torch.Size([6656])\n",
      "layers.49.ffn_norm.weight torch.Size([6656])\n",
      "layers.50.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.50.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.50.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.50.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.50.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.50.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.50.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.50.attention_norm.weight torch.Size([6656])\n",
      "layers.50.ffn_norm.weight torch.Size([6656])\n",
      "layers.51.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.51.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.51.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.51.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.51.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.51.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.51.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.51.attention_norm.weight torch.Size([6656])\n",
      "layers.51.ffn_norm.weight torch.Size([6656])\n",
      "layers.52.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.52.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.52.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.52.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.52.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.52.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.52.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.52.attention_norm.weight torch.Size([6656])\n",
      "layers.52.ffn_norm.weight torch.Size([6656])\n",
      "layers.53.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.53.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.53.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.53.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.53.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.53.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.53.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.53.attention_norm.weight torch.Size([6656])\n",
      "layers.53.ffn_norm.weight torch.Size([6656])\n",
      "layers.54.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.54.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.54.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.54.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.54.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.54.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.54.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.54.attention_norm.weight torch.Size([6656])\n",
      "layers.54.ffn_norm.weight torch.Size([6656])\n",
      "layers.55.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.55.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.55.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.55.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.55.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.55.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.55.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.55.attention_norm.weight torch.Size([6656])\n",
      "layers.55.ffn_norm.weight torch.Size([6656])\n",
      "layers.56.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.56.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.56.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.56.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.56.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.56.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.56.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.56.attention_norm.weight torch.Size([6656])\n",
      "layers.56.ffn_norm.weight torch.Size([6656])\n",
      "layers.57.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.57.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.57.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.57.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.57.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.57.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.57.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.57.attention_norm.weight torch.Size([6656])\n",
      "layers.57.ffn_norm.weight torch.Size([6656])\n",
      "layers.58.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.58.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.58.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.58.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.58.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.58.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.58.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.58.attention_norm.weight torch.Size([6656])\n",
      "layers.58.ffn_norm.weight torch.Size([6656])\n",
      "layers.59.attention.wq.weight torch.Size([6656, 6656])\n",
      "layers.59.attention.wk.weight torch.Size([6656, 6656])\n",
      "layers.59.attention.wv.weight torch.Size([6656, 6656])\n",
      "layers.59.attention.wo.weight torch.Size([6656, 6656])\n",
      "layers.59.feed_forward.w1.weight torch.Size([17920, 6656])\n",
      "layers.59.feed_forward.w2.weight torch.Size([6656, 17920])\n",
      "layers.59.feed_forward.w3.weight torch.Size([17920, 6656])\n",
      "layers.59.attention_norm.weight torch.Size([6656])\n",
      "layers.59.ffn_norm.weight torch.Size([6656])\n",
      "rope.freqs torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for k,v in whole_model_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b987a88d-c904-4c45-a578-5ed468f8e3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[model_0(\n",
       "   (tok_embeddings): VocabEmbedding()\n",
       "   (layers): ModuleList(\n",
       "     (0-14): 15 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_1(\n",
       "   (layers): ModuleList(\n",
       "     (0-14): 15 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_2(\n",
       "   (layers): ModuleList(\n",
       "     (0-14): 15 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " model_3(\n",
       "   (layers): ModuleList(\n",
       "     (0-14): 15 x TransformerBlock(\n",
       "       (attention): Attention(\n",
       "         (wq): noParallelLinear()\n",
       "         (wk): noParallelLinear()\n",
       "         (wv): noParallelLinear()\n",
       "         (wo): noParallelLinear()\n",
       "       )\n",
       "       (feed_forward): FeedForward(\n",
       "         (w1): noParallelLinear()\n",
       "         (w2): noParallelLinear()\n",
       "         (w3): noParallelLinear()\n",
       "       )\n",
       "       (attention_norm): RMSNorm()\n",
       "       (ffn_norm): RMSNorm()\n",
       "     )\n",
       "   )\n",
       "   (norm): RMSNorm()\n",
       "   (output): noParallelLinear()\n",
       " )]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lst = []\n",
    "for i in range(4):\n",
    "    model_lst.append(getattr(get_stage, \"model_\"+str(i))())\n",
    "model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd58857-577d-4521-8d5d-0d4f87c184ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(i)\n",
    "    state_dict = model_lst[i].state_dict()\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if \"layers\" in k:\n",
    "            new_k = k.lstrip(\"layers.\")\n",
    "            layer_id = int(new_k.split(\".\")[0])\n",
    "            tmp_k = new_k.lstrip(str(layer_id))\n",
    "            layer_id += 15*(i)\n",
    "            new_k = \"layers.\" + str(layer_id) + tmp_k\n",
    "            new_state_dict[k] = whole_model_dict[new_k]\n",
    "        else:\n",
    "            new_state_dict[k] = whole_model_dict[k]\n",
    "    model_lst[i].load_state_dict(new_state_dict)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0cb9247-9b19-46c8-9314-4b906d425b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [07:28<00:00, 112.21s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, model_ in enumerate(tqdm(model_lst)):\n",
    "    torch.save(model_.state_dict(), \"pp_ckpt/stage_\"+str(i)+\".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cechallenge",
   "language": "python",
   "name": "cechallenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
